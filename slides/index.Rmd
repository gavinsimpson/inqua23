---
title: "Applying statistical thinking to palaeo data through GAMs"
author: "Gavin Simpson"
institute: "Aarhus University"
date: "July 19, 2023"
output:
  xaringan::moon_reader:
    css: ["default", "https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css", "slides.css"]
    lib_dir: libs
    nature:
      titleSlideClass: ["inverse","middle","left","my-title-slide"]
      highlightStyle: "github"
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
      ratio: "16:9"
---

```{r setup, include=FALSE, cache=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache = TRUE, dev = "svg", echo = FALSE, message = FALSE, warning = FALSE,
                      fig.height=6, fig.width = 1.777777*6)

library("curl")
library("ggplot2")
library("dplyr")
library("tibble")
library("tidyr")
library("mgcv")
library("forcats")
library("mvnfast")
library("purrr")
library("gganimate")
library("gratia")
library("readr")
library("patchwork")
theme_set(theme_minimal(base_size = 14, base_family = "Fira Sans"))
library("readxl")
library("here")
library("pangaear")
library("janitor")
library("topicmodels")
library("analogue")
library("brms")
library("copula")
library("GJRM")
library("tibble")
library("ggtext")

## constats
anim_width <- 1900
anim_height <- anim_width / 1.77777777
anim_dev <- "png"
anim_res <- 300
```

```{r smooth-fun-animation, results = FALSE}
f <- function(x) {
    x^11 * (10 * (1 - x))^6 + ((10 * (10 * x)^3) * (1 - x)^10)
}

draw_beta <- function(n, k, mu = 1, sigma = 1) {
    rmvn(n = n, mu = rep(mu, k), sigma = diag(rep(sigma, k)))
}

weight_basis <- function(bf, x, n = 1, k, ...) {
    beta <- draw_beta(n = n, k = k, ...)
    out <- sweep(bf, 2L, beta, "*")
    colnames(out) <- paste0("f", seq_along(beta))
    out <- as_tibble(out)
    out <- add_column(out, x = x)
    out <- pivot_longer(out, -x, names_to = "bf", values_to = "y")
    out
}

random_bases <- function(bf, x, draws = 10, k, ...) {
    out <- rerun(draws, weight_basis(bf, x = x, k = k, ...))
    out <- bind_rows(out)
    out <- add_column(out, draw = rep(seq_len(draws), each = length(x) * k),
                      .before = 1L)
    class(out) <- c("random_bases", class(out))
    out
}

plot.random_bases <- function(x, facet = FALSE) {
    plt <- ggplot(x, aes(x = x, y = y, colour = bf)) +
        geom_line(lwd = 1, alpha = 0.75) +
        guides(colour = FALSE)
    if (facet) {
        plt + facet_wrap(~ draw)
    }
    plt
}

normalize <- function(x) {
    rx <- range(x)
    z <- (x - rx[1]) / (rx[2] - rx[1])
    z
}

set.seed(1)
N <- 500
data <- tibble(x     = runif(N),
               ytrue = f(x),
               ycent = ytrue - mean(ytrue),
               yobs  = ycent + rnorm(N, sd = 0.5))

k <- 10
knots <- with(data, list(x = seq(min(x), max(x), length = k)))
sm <- smoothCon(s(x, k = k, bs = "cr"), data = data, knots = knots)[[1]]$X
colnames(sm) <- levs <- paste0("f", seq_len(k))
basis <- pivot_longer(cbind(sm, data), -(x:yobs), names_to = "bf")
basis

set.seed(2)
bfuns <- random_bases(sm, data$x, draws = 20, k = k)

smooth <- bfuns %>%
    group_by(draw, x) %>%
    summarise(spline = sum(y)) %>%
    ungroup()

p1 <- ggplot(smooth) +
    geom_line(data = smooth, aes(x = x, y = spline), lwd = 1.5) +
    labs(y = "f(x)", x = "time") +
    theme_minimal(base_size = 16, base_family = "Fira Sans")

smooth_funs <- animate(
    p1 + transition_states(draw, transition_length = 4, state_length = 2) +
        ease_aes("cubic-in-out"),
    nframes = 200, height = anim_height, width = anim_width, res = anim_res,
    dev = anim_dev)

anim_save("resources/spline-anim.gif", smooth_funs)
```

.row[

.col-6[
### Slides & code

[bit.ly/inqua-talk-2023](https://bit.ly/inqua-talk-2023)

&copy; Simpson (2020&ndash;2023) [![Creative Commons Licence](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)
]

.col-6[

.center[

```{r, fig.align = "center", out.width="60%"}
knitr::include_graphics("resources/bit.ly_inqua-talk-2023.png")
```
]

]
]


### Land acknowledgment

* This work was started at University of Regina, on Treaty 4 lands

### Funding

.row[

.col-4[
.center[![:scale 80%](./resources/NSERC_C.svg)]
]

.col-4[
.center[![:scale 100%](./resources/fgsr-logo.jpg)]
]

.col-4[
.center[![:scale 100%](./resources/AUFF_logo_en.png)]
]

]


???

Thanks to Quinn and Jack for inviting me to give this talk

All my slides and code are on GitHub. The QR code here will take you to the slides - it will be shown again at the end as well. There will be other QR codes in the slides that will take you to extra information that I don't have time to cover today

This research was started while I was at the University of Regina, which is situated on the territories of the nay-hi-yuh-wuk (Cree; nÃªhiyawak), uh-nish-i-naa-payk (Salteaux; AnihÅ¡inÄpÄ“k), Dakota, Lakota, and Nakoda, and the homeland of the MÃ©tis/Michif Nation.

My work has been funded by NSERC, Faculty of Graduate Studies & Research at University of Regina, and Aarhus University Research Fund

---
class: inverse
background-image: url("resources/PS1920-1_0-750_sediment-core_hg.jpg")
background-position: right center
background-size: contain

# Statistical thinking & palaeo

Palaeo data are challenging

Important to grapple with these "problems"

Approaching data from the point of view of modelling process in our systems statistically

Estimate quantities of interest & quantify their uncertainty

.footnote[
    Image: Hannes Grobe, [CC BY 3.0](https://creativecommons.org/licenses/by/3.0), via Wikimedia Commons
]

???

We all know palaeo data are challenging to analyse, for a whole host of reasons & they don't play well with formal statistical time series methods

However, it is important to grapple with these problems and address them statistically

When I talk about statistical thinking and applying it to palaeo data, I'm thinking about having researchers approach the data from the view point of modelling them statistically, and using statistical models to estimate quantities of interest from a statistical model

---
class: inverse middle center subsection

# Generalized additive models

???

One such statistical model that I have found useful is the Generalized additive model; 
for example, they trivially handle the irregularity in time of most sequences

GAMs are a broad model class that are suited to modelling palaeo data

GAMs are models that represent the effects of variables on the response using smooth functions

---

# GAMs are built from splines

.center[![](resources/spline-anim.gif)]

???

GAMs use splines for the smooth functions

---

# Splines are formed from basis functions

```{r basis-functions, fig.height=6, fig.width = 1.777777*6}
ggplot(basis,
       aes(x = x, y = value, colour = bf)) +
    geom_line(lwd = 2, alpha = 0.5) +
    guides(colour = FALSE) +
    labs(x = "time", y = "b(x)") +
    theme_minimal(base_size = 16, base_family = "Fira Sans")
```

???

Splines are built up from basis functions

Here I'm showing a cubic regression spline basis with 10 knots/functions

If we weight each basis function and sum them up, we get a spline

---

# Weight basis functions &#8680; spline

```{r basis-function-animation, results = "hide"}
bfun_plt <- plot(bfuns) +
    geom_line(data = smooth, aes(x = x, y = spline),
              inherit.aes = FALSE, lwd = 1.5) +
    labs(x = "time", y = "f(x)") +
    theme_minimal(base_size = 16, base_family = "Fira Sans")

bfun_anim <- animate(
    bfun_plt +
        transition_states(draw, transition_length = 4, state_length = 2) +
        ease_aes("cubic-in-out"),
    nframes = 200, height = anim_height, width = anim_width, res = anim_res,
    dev = anim_dev)

anim_save("resources/basis-fun-anim.gif", bfun_anim)
```

.center[![](resources/basis-fun-anim.gif)]

???

If we choose different weights we get more wiggly splines

Each of the splines I showed you earlier are all generated from the same basis functions but using different weights

---

# Maximise penalised log-likelihood &#8680; &beta;

```{r basis-functions-anim, results = "hide"}
sm2 <- smoothCon(s(x, k = k, bs = "cr"), data = data, knots = knots)[[1]]$X
beta <- coef(lm(ycent ~ sm2 - 1, data = data))
wtbasis <- sweep(sm2, 2L, beta, FUN = "*")
colnames(wtbasis) <- colnames(sm2) <- paste0("F", seq_len(k))
## create stacked unweighted and weighted basis
basis <- as_tibble(rbind(sm2, wtbasis)) %>%
    add_column(x = rep(data$x, times = 2),
               type = rep(c("unweighted", "weighted"), each = nrow(sm2)),
               .before = 1L)

wtbasis <- as_tibble(rbind(sm2, wtbasis)) %>%
    add_column(x      = rep(data$x, times = 2),
               fitted = rowSums(.),
               type   = rep(c("unweighted", "weighted"), each = nrow(sm2))) %>%
    pivot_longer(-(x:type), names_to = "bf")
basis <- pivot_longer(basis, -(x:type), names_to = "bf")

p3 <- ggplot(data, aes(x = x, y = ycent)) +
    geom_point(aes(y = yobs), alpha = 0.2) +
    geom_line(data = basis,
              mapping = aes(x = x, y = value, colour = bf),
              lwd = 1, alpha = 0.5) +
    geom_line(data = wtbasis,
              mapping = aes(x = x, y = fitted), lwd = 1, colour = "black", alpha = 0.75) +
    guides(colour = FALSE) +
    labs(y = "f(x)", x = "time") +
    theme_minimal(base_size = 16, base_family = "Fira Sans")

crs_fit <- animate(p3 + transition_states(type, transition_length = 4, state_length = 2) + 
                   ease_aes("cubic-in-out"),
                   nframes = 100, height = anim_height, width = anim_width, res = anim_res,
                   dev = anim_dev)

anim_save("./resources/gam-crs-animation.gif", crs_fit)
```

.center[![](resources/gam-crs-animation.gif)]

???

Fitting a GAM to data involves finding the weights for the basis functions that produce a spline that fits the data best, subject to some constraints to avoid over fitting

---
class: inverse middle center subsection

# Beyond the mean

---
background-image: url("resources/7sq9ip.jpg")
background-position: right center
background-size: auto

# Distributional GAMs

.row[

.col-7[

We mostly model the mean

But other parameters are important

...and of ecological interest

Also a way to handle the varying amount of time averaging (precision) in non-annually laminated sediments

]

.col-5[
&nbsp;
]

]

---

# Variance

Intuitive & key descriptor of ecosystem state

.row[
.col-6[
.center[
```{r high-resilience}
knitr::include_graphics('./resources/scheffer-2012-high-resilience.jpg')
```
]
]

.col-6[
.center[
```{r low-resilience}
knitr::include_graphics('./resources/scheffer-2012-low-resilience.jpg')
```
]
]
.col-12[
.small[Source: Scheffer *et al* Science (2012)]
]
]

???

The variance is an important measure of ecosystem state

Ecologists have linked variance with coexistence of populations and variance can be interpreted in terms of resilience theory

Here I'm showing two cartoons;

A: in the high resilience state the ecosystem returns rapidly to the equilibrium following perturbation and hence has low variance

B: in the low resilience state, where the basin of attraction has become shallower, the ecosystem takes longer to return to equilibrium following perturbation and hence exhibits high variance

Variance, however, is much harder to estimate from data

---

# Distributional GAMs

.row[

.col-6[

See my talk @ INQUA 2019 in Dublin
]

.col-6[
```{r}
knitr::include_graphics("resources/bit.ly_inquaresilience.png")
```
]

]

---
class: inverse middle center subsection

# Rates of change

```{r roc-setup, echo = FALSE, results = "hide"}
# load the data from Pangaea
res <- pg_data(doi = "10.1594/PANGAEA.857573")
foram <- res[[1]]$data |>
    setNames(c("Depth", "Age_AD", "Age_kaBP", "d18O"))

max_agebp <- with(foram, max(-Age_kaBP))
min_agebp <- with(foram, min(-Age_kaBP))

foram <- foram |>
    mutate(t = (-Age_kaBP - min_agebp) / (max_agebp - min_agebp),
    neg_age = -Age_kaBP)

# some plotting constants
ylabel <- expression(delta^{18} * O ~ "[â€° VPDB]")
xlabel <- "Age [ka BP]"

# fit the model
m <- gam(d18O ~ s(neg_age, k = 100, bs = "ad"), data = foram, method = "REML")
```

---

# Rates of change

Knowing how quickly systems changed in the past can be used a to put current or future rates of change in context

Long history of RoC analysis in palaeo (Birks & Gordon, 1985) & recent updates ([RRatepol ðŸ“¦ OndÅ™ej Mottl](https://doi.org/10.1016/j.revpalbo.2021.104483))

An alternative is to use the _derivatives_ of temporal smooths

???

Most options are not grounded in a statistical model &mdash; making inference difficult

---

# Finite differences

We don't have an equation for splines, so mathy derivatives are out&hellip;

But we can use _finite differences_ to estimate them

```{r finite-differences-plot, fig="hold", echo=FALSE, results="hide", fig.height = 5, fig.width = 1.77777*6, out.width = "90%", fig.align = "center"}
## Daily Central England Temperature Series
URL <- "https://www.metoffice.gov.uk/hadobs/hadcet/data/meantemp_daily_totals.txt"

cet <- read_table(URL, skip = 1L, col_types = "Dd")
cet <- cet |>
    janitor::clean_names() |>
    mutate(doy = as.numeric(format(date, "%j")),
        year = as.numeric(format(date, "%Y")),
        time = seq_len(nrow(cet)) / nrow(cet),
        lag1 = dplyr::lag(value))

knots <- list(doy = c(0.5, 366.5))
# Intentionall lower k on year smooth just for the finite diff figure
m2 <- bam(value ~ s(doy, bs = "cc", k = 20) + s(year, k = 10, bs = "cr"),
    data = cet, method = "fREML", knots = knots,
    nthreads = 4, discrete = TRUE)

want <- seq(1, nrow(cet), length.out = 200)
pdat <- with(cet,
    data.frame(year = year[want], date = date[want],
        doy = doy[want]))
pdat <- data_slice(m2, year = evenly(year, n = 200))
fv <- fitted_values(m2, data = pdat) # predict(m2, newdata = pdat, type = "terms", se.fit = TRUE)

ylab <- "Response"

## Illustrate the finite differences method
layout(matrix(1:2, ncol = 2))
op <- par(mar = c(5,4,1,1) + 0.1)
plot(fitted ~ year, data = fv, type = "n", ylab = ylab)
lines(fitted ~ year, data = fv, lwd = 4)
## choose 5 points
take <- floor(seq.int(1, nrow(pdat), length = 5))
abline(v = fv$year[take], lty = "dashed", col = "red")
lines(fitted ~ year, data = fv, subset = take, col = "red", lwd = 2,
      type = "o", pch = 16, cex = 1.5)
plot(fitted ~ year, data = fv, type = "n", ylab = ylab)
lines(fitted ~ year, data = fv, lwd = 4)
## choose 20 points, so finer
take2 <- floor(seq.int(1, nrow(pdat), length = 20))
abline(v = fv$year[take2], lty = "dashed", col = "forestgreen")
lines(fitted ~ year, data = fv, subset = take2, col = "forestgreen", lwd = 2,
      type = "o", pch = 16, cex = 1.5)
par(op)
layout(1)
```

???

We do actually have an equation but we'd need to figure it out and that is tedious and hard

---

# Rate of change

For a single variable, this is relatively simple

```{r display-foram-data, dependson = "roc-setup", out.width = "80%", fig.align = "center"}
ggplot(foram, aes(y = d18O, x = Age_kaBP)) +
    geom_path() +
    scale_x_reverse(sec.axis = sec_axis( ~ 1950 - (. * 1000),
        name = "Year [CE]")) +
    labs(y = ylabel, x = xlabel)
```

[Taricco, _et al_ (2016) _Scientific Data_ **3**, 160042](https://doi.org/10.1038/sdata.2016.42)

???

Gulf of Taranto (Ionian Sea)

Oxygen isotope composition Î´18O of planktonic foraminifera in one of the cores extracted from the Gallipoli Terrace

---

# Estimate the trend

```{r display-foram-gam, dependson = "roc-setup"}
ds <- data_slice(m, neg_age = evenly(neg_age, 300))
fv <- fitted_values(m, data = ds) |>
    mutate(Age_kaBP = -neg_age) # (t * (max_agebp - min_agebp) + min_agebp))


fv |>
    ggplot(aes(y = fitted, x = Age_kaBP)) +
        geom_point(data = foram, aes(y = d18O, x = Age_kaBP), alpha = 0.5) +
        geom_ribbon(aes(x = Age_kaBP, ymin = lower, ymax = upper), alpha = 0.2) +
        geom_line(aes(x = Age_kaBP, y = fitted),
                  linewidth = 1) +
        scale_x_reverse(sec.axis = sec_axis( ~ 1950 - (. * 1000),
            name = "Age [CE]")) +
        #scale_y_reverse() +
        labs(y = ylabel, x = xlabel)
```

---

# Compute derivatives = RoC

```{r display-foram-derivs, dependson = "roc-setup"}
# compute first derivative of the smooth
fd <- derivatives(m, data = ds, type = "central")
fd |>
    mutate(data = -data) |>
    draw(add_change = TRUE, change_type = "sizer", lwd_change = 3) &
    geom_hline(yintercept = 0, col = "red") &
    labs(x = xlabel) &
    scale_x_reverse()
```

???

Above the 0 line d18O is increasingm below the line it is decreasing

We can compute the uncertainty in the derivative (RoC) that arises from the uncertainty in the estimated trend

Where the uncertainty band on the derivative excludes 0 we might also infer the istopes are changing in a way that is inconsistent with no change

---

# Compositional rate of change

Compositional data are counts from a total &mdash; closed compositional data

Can't use a Hierarchical GAM for these data

Multinomial or Dirichlet models would be the correct statsy way to go

But the number of taxa is problematic

???

Doing this with compositional data is a bit more difficult

Because the data are close compositional due to the fixed sample total count we can't use hierarchical GAMs

Fitting a multinomial or Dirichlet GAM would be the way to go, but we typically have too many taxa

---

# Dimension reduction

Need dimension reduction, but not PCA or CA etc

We don't want to break non-linear trends over multiple axes

Use a topic model, which

1. finds groups of taxa that tend to occur together &mdash; *associations*
2. models each sample as different proportions of the *associations*

More ecologically realistic

???

So we need to reduce the dimensionalty in some way

PCA and CA are not useful as they split up trends into separate, uncorrelated components

Instead we could use a topic model

--

After application of the topic model we are typically fitting a Dirichlet model to the proportions of a few (<= 10) associations of species

---

# Topic models &mdash; moar

PaleoEcoGen Seminar 2022 [slides](https://bit.ly/paleoecogen-gavin) & [recording](https://bit.ly/paleoecogen-video)

.row[
.col-6[
```{r}
knitr::include_graphics("resources/bit.ly_paleoecogen-gavin.png")
```
]

.col-6[
```{r}
knitr::include_graphics("resources/bit.ly_paleoecogen-video.png")
```

]
]

???

I don't have time to explain topic models here, but I recently gave in the PAGES PaleoEcoGen seminar series that includes an explanation of them. The QR codes are links to the slides and the zoom recording of that talk

```{r topic-model-setup}
## Load data
aber <- read_rds(here("data", "abernethy-count-data.rds"))
## or:
# aber <- read_rds("https://bit.ly/abercount")

## take a subset of spp
take <- c("BETULA", "PINUS_SYLVESTRIS", "ULMUS", "QUERCUS", "ALNUS_GLUTINOSA",
          "CORYLUS_MYRICA", "SALIX", "JUNIPERUS_COMMUNIS", "CALLUNA_VULGARIS",
          "EMPETRUM", "GRAMINEAE", "CYPERACEAE", "SOLIDAGO_TYPE",
          "COMPOSITAE_LIGULIFLORAE", "ARTEMISIA",
          "CARYOPHYLLACEAE_UNDIFFERENTIATED",
          "SAGINA", "SILENE_CF_S_ACAULIS", "CHENOPODIACEAE", "EPILOBIUM_TYPE",
          "PAPILIONACEAE_UNDIFFERENTIATED", "ANTHYLLIS_VULNERARIA",
          "ASTRAGALUS_ALPINUS", "ONONIS_TYPE", "ROSACEAE_UNDIFFERENTIATED",
          "RUBIACEAE", "RANUNCULACEAE_UNDIFFERENTIATED", "THALICTRUM",
          "RUMEX_ACETOSA_TYPE", "OXYRIA_TYPE", "PARNASSIA_PALUSTRIS",
          "SAXIFRAGA_GRANULATA", "SAXIFRAGA_HIRCULUS_TYPE", "SAXIFRAGA_NIVALIS",
          "SAXIFRAGA_OPPOSITIFOLIA", "SAXIFRAGA_UNDIFFERENTIATED", "SEDUM",
          "URTICA", "VERONICA")
## Don't do this!
##take <- c(1,2,3,4,6,10,11,12,14,15,39,40,41,42,43,46,49,50,53,54,57,58,59,60,67,
##          69,70,72,74,75,83,85,86)
aber <- aber[, take]
## are any columns now all zeroes?
all_missing <- unname(vapply(aber, function(x) all(is.na(x)), logical(1)))
## drop those with all NA
aber <- aber[, !all_missing]
## change all the NA to 0s
aber <- tran(aber, method = "missing")
## check that all remaining values are numeric
stopifnot(all(vapply(aber, data.class, character(1), USE.NAMES = FALSE) == "numeric"))
## check all columns still have at least 1 positive count
cs <- colSums(aber) > 0L
aber <- aber[, cs]
names(aber) <- tolower(names(aber))
## aber ages
aber_age <- read_rds(here("data", "abernethy-sample-age.rds"))
## or:
# aber_age <- read_rds("https://bit.ly/aberage")

## Models to fit
k <- 2:10 # 2, 3, ... 10 associations / groups
## setting the same random seed for each model
reps <- length(k)
ctrl <- replicate(reps, list(seed = 42), simplify = FALSE)
## repeat the data n times to facilitate `mapply`
aberrep <- replicate(reps, aber, simplify = FALSE)
# fit the sequence of topic models
tms <- mapply(LDA, k = k, x = aberrep, control = ctrl)

## extract model fit in terms of BIC and plot
# plot(k, sapply(tms, AIC, k = log(nrow(aber)))) # not in talk

## so 6 groups looks OK
k_take <- 6
## which is the 5 group model?
k_ind <- which(k == k_take)
## could also selected purely on BIC terms...
k_bic <- which.min(sapply(tms, AIC, k = log(nrow(aber))))
## but we'll take the model with 5 groups
aber_lda <- tms[[k_ind]]

## extract the posterior fitted distribution of the model
aber_posterior <- posterior(aber_lda)
## topic proportions
aber_topics <- aber_posterior$topics
## term proportions
aber_terms <- aber_posterior$terms

# Prep the data for modelling
# aber_topics needs to be a data frame with suitable names, currently a matrix
# with invalid names
topic_df <- tibble::as_tibble(aber_topics) |>
    setNames(paste0("assoc", seq_len(k_take))) |>
    bind_cols(aber_age) |>
    rename(age = Age) |>
    mutate(neg_age = -age,
        t = 1 - ((age - min(age)) / (max(age) - min(age))))

# topic_df |>
#     pivot_longer(cols = c(-age, -neg_age, -t),
#         names_to = "association",
#         names_pattern = "assoc([[:digit:]]{1})",
#         values_to = "proportion") |>
#     ggplot(aes(x = age, y = proportion, colour = association)) +
#         geom_point() +
#         scale_x_reverse()

# Fit the brms dirichlet model
bind <- function(...) cbind(...)
bind <- cbind

fml1 <- brmsformula(bind(assoc1, assoc2, assoc3, assoc4, assoc5, assoc6) ~
    s(t, k = 10))
fml2 <- brmsformula(bind(assoc1, assoc2, assoc3, assoc4, assoc5, assoc6) ~
    s(t, k = 8), phi ~ s(t, k = 15))

if (FALSE) { ## Don't refit in the slides! Needs cmdstanr backend - memory!
    m <- brm(fml1, data = topic_df, family = dirichlet(refcat = "assoc3"),
        chains = 4, cores = 4, control = list(adapt_delta = 0.975), iter = 5000,
        backend = "cmdstanr", seed = 15)

    write_rds(m, "models/aber-dirichlet.rds")

    m2 <- brm(fml2, data = topic_df, family = dirichlet(refcat = "assoc3"),
        chains = 4, cores = 4, control = list(adapt_delta = 0.99), iter = 5000,
        backend = "cmdstanr", seed = 15)

    write_rds(m2, "models/aber-dirichlet-phi.rds")
} else {
    # instead load the model files
    # load the model (saved - not in repo!
    m  <- read_rds(here("models/aber-dirichlet.rds"))
    m2 <- read_rds(here("models/aber-dirichlet-phi.rds"))
}

# new data for plotting & derivatives
new_df <- with(topic_df, tibble(age = seq(min(age), max(age), length = 100))) |>
    mutate(neg_age = -age,
        t = 1 - ((age - min(age)) / (max(age) - min(age))))

# fitted values
if (FALSE) { # something weird going on if you process models locally, fitted elsewhere
    fv1 <- fitted(m, newdata = new_df, scale = "response")
    fv2 <- fitted(m2, newdata = new_df, scale = "response")

    fv1_samps <- fitted(m, newdata = new_df, scale = "response",
        summary = FALSE)
    fv2_samps <- fitted(m2, newdata = new_df, scale = "response",
        summary = FALSE)

    write_rds(fv1, "models/aber-dirichlet-fitted-summary.rds")
    write_rds(fv2, "models/aber-dirichlet-phi-fitted-summary.rds")
    write_rds(fv1_samps, "models/aber-dirichlet-fitted-samples.rds")
    write_rds(fv2_samps, "models/aber-dirichlet-phi-fitted-samples.rds")
} else { # so load the posterior draws saved previously
    fv1 <- read_rds(here("models/aber-dirichlet-fitted-summary.rds"))
    fv2 <- read_rds(here("models/aber-dirichlet-phi-fitted-summary.rds"))
    fv1_samps <- read_rds(here("models/aber-dirichlet-fitted-samples.rds"))
    fv2_samps <- read_rds(here("models/aber-dirichlet-phi-fitted-samples.rds"))
}

# need to extract and wrangle the fvs into the required format
`extract_fv` <- function(x, data, k) {
    prob <- array_to_long_tibble(x) # posterior mean (default for fitted)
    lwr  <- array_to_long_tibble(x, var = "lower") # 2.5th quantile
    upr  <- array_to_long_tibble(x, var = "upper") # 97.5th quantile
    out <- bind_cols(prob, lwr[, "lower"], upr[, "upper"])

    # add on the data var
    out <- tibble::add_column(out,
        age = rep(data$age, each = k),
        neg_age = rep(data$neg_age, each = k),
        t = rep(data$t, each = k), .before = 1L)
    out
}

`array_to_long_tibble` <- function(x, var = c("proportion", "lower", "upper")) {
    var <- match.arg(var)
    take <- case_when(
        var == "proportion" ~ 1,
        var == "lower" ~ 3,
        var == "upper" ~ 4)
    out <- as_tibble(x[, take, ]) |>
        pivot_longer(cols = everything(), # pivot
            names_pattern = "assoc([[:digit:]]{1})",
            names_to = "association",
            values_to = var)
    out
}

fvs <- extract_fv(fv2, data = new_df, k = 6)

topic_df_long <- topic_df |>
    pivot_longer(cols = c(-age, -neg_age, -t),
        names_to = "association",
        names_pattern = "assoc([[:digit:]]{1})",
        values_to = "proportion")

# derivatives
eps <- 1e-7

deriv_df_bkw <- with(topic_df, tibble(age = seq(min(age), max(age), length = 200))) |>
    mutate(neg_age = -age,
        t = 1 - ((age - min(age)) / (max(age) - min(age))),
        t = t - (eps / 2))

deriv_df_fwd <- with(topic_df, tibble(age = seq(min(age), max(age), length = 200))) |>
    mutate(neg_age = -age,
        t = 1 - ((age - min(age)) / (max(age) - min(age))),
        t = t + (eps / 2))

deriv_df <- deriv_df_fwd |>
    bind_rows(deriv_df_bkw)

if (FALSE) {
    fd1_samps <- fitted(m, newdata = deriv_df, scale = "response", summary = FALSE)
    fd2_samps <- fitted(m2, newdata = deriv_df, scale = "response", summary = FALSE)

    write_rds(fd1_samps, "models/aber-dirichlet-fitted-samples-derivs.rds")
    write_rds(fd2_samps, "models/aber-dirichlet-phi-fitted-samples-derivs.rds")
} else {
    fd1_samps <- read_rds(here("models/aber-dirichlet-fitted-samples-derivs.rds"))
    fd2_samps <- read_rds(here("models/aber-dirichlet-phi-fitted-samples-derivs.rds"))
}
```

---

# Dirichlet GAM

.row[

.col-6[
% sand, silt clay

Such data lie on a simplex; $k$ classes have $k-1$ dimensions

Fitted using brms ðŸ“¦ & Stan
]

.col-6[
```{r include-dirichlet-figure}
knitr::include_graphics("resources/page1-1500px-Dirichlet.pdf.jpg")
```
]
]

.small[
```{r dirichlet-explanation}
head(aber_topics)
```
]

???

Having reduced the many taxa to a few associations, we model their proportions with a Dirichlet GAM

The classic example of Dirichlet distributed data is % sand, silt, and clay. You only need two of these to deduce all three proportions

Technically we say such data (samples) lie on a simplex with 1 fewer dimensions than "types"

---

# Abernethy Forest

```{r abernethy-strat-plot, fig.align = "center", out.width = "90%"}
aber_tmp <- aber |>
    bind_cols(aber_age) |>
    as.data.frame()
Stratiplot(Age ~ .,
    data = chooseTaxa(aber_tmp, n.occ = 5, max.abun = 50),
    type = "poly")
```

Birks & Mathewes (1978) _New Phytologist_ **80**, 455-484.

???

To illustrate the approach I'll use the classic late glacial pollen data from Abernethy Forest, Scotland, of Birks and Mathewes

---

# Trends in species associations

```{r plot-dirichlet-trends, dependson = -2}
fvs |>
    ggplot(aes(x = age, y = proportion, colour = association)) +
    geom_ribbon(aes(x = age, ymin = lower, ymax = upper,
        fill = association), inherit.aes = FALSE, alpha = 0.2) +
    geom_line(linewidth = 1.5) +
    # geom_point(data = topic_df_long,
    #     mapping = aes(x = age, y = proportion, colour = association)) +
    scale_x_reverse() +
    labs(x = "Age (years BP)", y = "Relative abundance")
```

???

Firstly, we estimate the trends in the relative composition of each association of taxa using the Dirichlet GAM

---

# Compute derivatives of trends

```{r plot-dirichlet-derivatives, dependson = -3}
# used more points (200) for the derivatives samples... :cry
new_df <- with(topic_df, tibble(age = seq(min(age), max(age), length = 200))) |>
    mutate(neg_age = -age,
        t = 1 - ((age - min(age)) / (max(age) - min(age))))
fd_dif <- (fd2_samps[, 1:200, ] - fd2_samps[, 201:400, ]) / eps

# work on the columns
fd_med <- apply(fd_dif, 2:3, quantile, probs = 0.5) |>
    as_tibble() |>
        pivot_longer(cols = everything(), # pivot
            names_pattern = "assoc([[:digit:]]{1})",
            names_to = "association",
            values_to = "derivative")

fd_lwr <- apply(fd_dif, 2:3, quantile, probs = 0.025) |>
    as_tibble() |>
    pivot_longer(cols = everything(), # pivot
        names_pattern = "assoc([[:digit:]]{1})",
        names_to = "association",
        values_to = "lower")

fd_upr <- apply(fd_dif, 2:3, quantile, probs = 0.975) |>
    as_tibble() |>
        pivot_longer(cols = everything(), # pivot
            names_pattern = "assoc([[:digit:]]{1})",
            names_to = "association",
            values_to = "upper")

fd_df <- fd_med |>
    bind_cols(fd_lwr[, "lower"]) |>
    bind_cols(fd_upr[, "upper"]) |>
    tibble::add_column(age = rep(new_df$age, each = k_take),
        neg_age = rep(new_df$neg_age, each = k_take),
        t = rep(new_df$t, each = k_take), .before = 1L)

fd_df |>
    ggplot(aes(x = age, y = derivative, colour = association)) +
    geom_line(linewidth = 1.5) +
    scale_x_reverse() +
    labs(x = "Age (years BP)", y = "Derivative")
```

???

Then we compute the derivatives of those trends using finite differences

---

# Take the absolute values |d|

```{r plot-dirichlet-abs-derivatives, dependson = -1}
fd_df |>
    ggplot(aes(x = age, y = abs(derivative), colour = association)) +
    geom_line(linewidth = 1.5) +
    scale_x_reverse() +
    labs(x = "Age (years BP)", y = "| derivative |")
```

???

We take the absolute value of the derivatve as decreases are as important as increases

---

# Sum |d| at each time point - RoC

```{r plot-dirichlet-roc, dependson = -2}
fd_df |>
    group_by(age) |>
    summarise(rate_of_change = sum(abs(derivative))) |>
    ggplot(aes(x = age, y = rate_of_change)) +
    geom_line(linewidth = 1.5) +
    scale_x_reverse() +
    labs(x = "Age (years BP)", y = "Rate of compositional change")

```

???

And then we sum the absolute derivative to get our estimate of the rate of change or turnover
---

# 

```{r compare-with-rratepol}
library("RRatepol")

aber2 <- cbind.data.frame(sample_id = as.character(seq_len(nrow(aber))), aber)
rownames(aber2) <- NULL
aber_age2 <- aber_age |> rename(age = Age) |>
    tibble::add_column(aber_age,
        sample_id = as.character(seq_len(nrow(aber_age))), .before = 1L) |>
    as.data.frame()
rownames(aber_age2) <- NULL

aber_rp <- estimate_roc(
    data_source_community = aber2,
    data_source_age = aber_age2,
    smooth_method = "age.w",
    dissimilarity_coefficient = "chord",
    working_units = "levels",
    bin_size = 200
    )

library("ggtext")
fd_df |>
    group_by(age) |>
    summarise(rate_of_change = sum(abs(derivative))) |>
    ggplot(aes(x = age, y = rate_of_change)) +
    geom_line(linewidth = 1.5, colour = "#56B4E9") +
    geom_line(data = aber_rp,
        mapping = aes(x = Age, y = ROC * 12), colour = "#E69F00",
        linewidth = 1.5) +
    scale_x_reverse() +
        labs(x = "Age (years BP)", y = "Rate of compositional change",
            title = "RoC based on <span style = 'color:#56B4E9;'>Dirichlet GAM</span> and <span style = 'color:#E69F00;'>RRatepol</span>") +
    theme(plot.title = element_markdown(size = 26),
    plot.title.position = "plot")
```

???

If we compare with a basic run of RRatepol, we see similar features in the rate of change, but there are differences in some of the detail that are worthy of further investigation

---
class: inverse middle center subsection

# Correlating time series

???

The last example I want to show is how we can use GAMs to estimate the correlation between time series

---

# How correlated are variables in time?

```{r copula-gamlss-load-plot-data}
# load data
sw <- readr::read_csv(here("data", "small-water-isotopes.csv"),
    col_types = "ddddddd")

# create a tidy format
sw_tidy <- sw |>
    pivot_longer(c(-depth, -year),
        names_to = "variable", values_to = "values")

# plot the data
ok_pal <- palette.colors(3)[-1] |> unname()

iso_lab <- function(x) {
    c("d13c" = "delta^{13}*C", "d15n" = "delta^{15}*N")
}

pm_lab <- "\u2030"

iso_plt <- sw_tidy |>
    filter(variable %in% c("d13c", "d15n")) |>
    ggplot(aes(x = year, y = values, group = variable, colour = variable)) +
    geom_point(show.legend = FALSE, size = 3) +
    facet_grid(variable ~ ., scales = "free_y",
      labeller = as_labeller(iso_lab, default = label_parsed)) +
    scale_colour_discrete(type = ok_pal) +
    labs(x = NULL, y = pm_lab)
iso_plt
```

```{r fit-copulas}
mu_13c    <- d13c ~ s(year, k = 10)
mu_15n    <- d15n ~ s(year, k = 10)
sigma_13c <- ~ 1 # + s(year, k = 4)
sigma_15n <- ~ 1 # +  s(year, k = 4)
theta     <- ~ s(year, k = 10)
nu        <- ~ 1
fml <- list(mu_13c, mu_15n, sigma_13c, sigma_15n, theta)
fml_t <- c(fml, nu)

# different copulas
m_cop_N     <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "N", gamlssfit = TRUE)
m_cop_F     <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "F", gamlssfit = TRUE)
m_cop_AMH   <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "AMH", gamlssfit = TRUE)
m_cop_FGM   <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "FGM", gamlssfit = TRUE)
m_cop_C0    <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "C0", gamlssfit = TRUE)
m_cop_C270  <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "C270", gamlssfit = TRUE)
m_cop_G0    <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "G0", gamlssfit = TRUE)
m_cop_G90   <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "G90", gamlssfit = TRUE)
m_cop_G180  <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "G180", gamlssfit = TRUE)
m_cop_G270  <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "G270", gamlssfit = TRUE)
m_cop_J0    <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "J0", gamlssfit = TRUE)
m_cop_J90   <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "J90", gamlssfit = TRUE)
m_cop_J270  <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "J270", gamlssfit = TRUE)
m_cop_PL    <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "PL", gamlssfit = TRUE)
m_cop_HO    <- gjrm(fml, data = sw, margins = c("N", "N"), model = "B",
                    copula = "HO", gamlssfit = TRUE)
m_cop_T     <- gjrm(fml_t, data = sw, margins = c("N", "N"), model = "B",
    copula = "T", gamlssfit = TRUE)
```

???

Here I'm showing a stable isotope record of 13C and 15N in bulk organic matter from Small Water, a small corrie lake on the edge of the English Lake District

We were interested in investigating biogeochemical changes due to atmospheric deposition over the last couple of hundred years

---

# Copulas

Copula distributional GAM

Copulas bind two models/distributions together

Define a general way to think about dependence between random variables

Starting to be used in ecology:

* Popovic, Hui, Warton, 2018. J. Multivar. Anal. 165, 86â€“100. doi: 10/dzx9
* Anderson et al 2019. Ecol. Evol. 44, 182. doi: 10/dzzb

???

To correlate the two records we need to model them and their correlation over time

We will use a copula distributional GAM for this

The new bit, the copula, is a function that binds distributions or models together in a way that estimates the dependence between the two data series

Copulas are a general way to think about the dependence between variables that extends the idea of bivariate normality and Pearson's product moment correlation

---

# Examples of copulas

```{r copula-examples}
## Example of different copulas with Gaussian marginals
tau <- -0.7
th.n <- iTau(normalCopula(), tau = tau)
th.t <- iTau(tCopula(df = 3), tau = tau)
th.c <- iTau(rotCopula(claytonCopula(), flip = c(TRUE, FALSE)), tau = tau)
th.g <- iTau(rotCopula(gumbelCopula(), flip = c(TRUE, FALSE)), tau = tau)

## sample from objects
set.seed(271)
n <- 2000
N01m <- list(list(mean = 0, sd = 1), list(mean = 0, sd = 1))
X.n <- rMvdc(n, mvdc = mvdc(normalCopula(th.n),    c("norm", "norm"), N01m))
X.t <- rMvdc(n, mvdc = mvdc(tCopula(th.t, df = 3), c("norm", "norm"), N01m))
X.c <- rMvdc(n, mvdc = mvdc(rotCopula(claytonCopula(th.c),
    flip = c(TRUE, FALSE)), c("norm", "norm"), N01m))
X.g <- rMvdc(n, mvdc = mvdc(rotCopula(gumbelCopula(th.g),
    flip = c(TRUE, FALSE)), c("norm", "norm"), N01m))

## put into something I can plot with ggplot
cop_names <- c("Normal", "t", "Clayton", "Gumbel")
cops <- tibble(x1 = c(X.n[, 1], X.t[, 1], X.c[, 1], X.g[, 1]),
               x2 = c(X.n[, 2], X.t[, 2], X.c[, 2], X.g[, 2]),
               copula = factor(rep(cop_names, each = n), levels = cop_names))

## colours
pal <- c(Normal = "#0077bb", t = "#009988", Clayton = "#cc3311",
    Gumbel = "#ee3377")
ggplot(cops, aes(x = x1, y = x2, colour = copula)) +
    geom_point(alpha = 0.3) +
    facet_wrap(~ copula, nrow = 1) +
    scale_colour_manual(values = pal, guide = FALSE) +
    theme_bw(base_size = 14, base_family = 'Fira Sans') +
    theme(strip.text = element_text(size = rel(1), face = "bold", hjust = 0),
          plot.title = element_text(face = "bold")) +
    labs(caption = bquote("Kendall's" ~ tau == .(tau)))
```

???

Here I am showing different copulas to link two normally distributed data sets

The Normal and t copula are familiar and relate directly to the correlation coefficients

The Clayton and Gumbel copulas are different; they model differences in tail dependences, where the data can be uncorrelated in one tail and correlated in the other

---

# Copula distributional GAMs

Model the parameters (mean and variance) of each stable isotope time series, plus the copula parameter $\theta$ as an extra linear predictor

.small[
$$\begin{align}
F(y_{\mathsf{\delta^{15}N}_{i}}, y_{\mathsf{\delta^{13}C}_{i}} | \vartheta^{k} ) & = \mathcal{C}(F_{\mathsf{\delta^{15}N}_{i}}(y_{\mathsf{\delta^{15}N}_{i}} | \mu_{\mathsf{\delta^{15}N}_{i}}, \sigma_{\mathsf{\delta^{15}N}_{i}}), F_{\mathsf{\delta^{13}C}_{i}}(y_{\mathsf{\delta^{13}C}_{i}} | \mu_{\mathsf{\delta^{13}C}_{i}}, \sigma_{\mathsf{\delta^{13}C}_{i}}), \theta) \\
y_{\mathsf{\delta^{15}N}_{i}} & \sim \mathcal{N}(\mu_{\mathsf{\delta^{15}N}_{i}}, \sigma_{\mathsf{\delta^{15}N}_{i}}) \\
y_{\mathsf{\delta^{13}C}_{i}} & \sim \mathcal{N}(\mu_{\mathsf{\delta^{13}C}_{i}}, \sigma_{\mathsf{\delta^{13}C}_{i}})
\end{align}$$
]

$$\begin{align}
\log(\mu_{\mathsf{\delta^{15}N}_{i}}) & = \beta^{\mu_{\mathsf{\delta^{15}N}}}_0 + f^{\mu_{\mathsf{\delta^{15}N}}}(\text{Year}_i) \\
\log(\mu_{\mathsf{\delta^{13}C}_{i}}) & = \beta^{\mu_{\mathsf{\delta^{13}C}}}_0 + f^{\mu_{\mathsf{\delta^{13}C}}}(\text{Year}_i) \\
\log(\sigma_{\mathsf{\delta^{15}N}_{i}}) & = \beta^{\sigma_{\mathsf{\delta^{15}N}}}_0 \\
\log(\sigma_{\mathsf{\delta^{13}C}_{i}}) & = \beta^{\sigma_{\mathsf{\delta^{13}C}}}_0 \\
g(\theta_i)   & = \beta^{\theta}_0 + f^{\theta}(\text{Year}_i)
\end{align}$$

???

This is the nastiest slides I'll show, but the important point is in these lines at the bottom

We note that I am modelling the means of both records with smooths of time, that I am assuming no change in variance (precision), and most importantly for this example, that I am modelling the correlation between to two series using a smooth function of time

---

# Copulas &mdash; AIC

.row[

.col-6[

Fit a range of copula types

* Bivariate normal
* Frank
* Ali-Mikhail-Haq
* Farlie-Gumbel-Morgenstern
* Plackett
* Student t
* Hougaard
* Clayton (0, 270)
* Gumbel (0, 90, 180, 270)
* Joe (0, 90)

AIC to select best-fitting model

]

.col-6[

.small[
```{r copula-aic-tab}
AIC(m_cop_N, m_cop_F, m_cop_AMH, m_cop_FGM, m_cop_PL, m_cop_HO, m_cop_T,
    m_cop_C0, m_cop_C270, # m_cop_C90,
    m_cop_G0, m_cop_G90, m_cop_G180, m_cop_G270,
    m_cop_J0, m_cop_J90 # , m_cop_J270)
    ) |>
    data.frame() |>
        tibble::rownames_to_column(var = "model") |>
        arrange(AIC) |>
        slice_min(n = 10, order_by = AIC) |>
    knitr::kable()
```
]

]

]

???

There are many kinds of copula to choose from and selecting just one that matches your data makes model selection more difficult

Instead we fit many different kinds of copula and choose the one (or several) that are most compatible with the data, say using AIC

Here we see that Gumbel copula (rotated 270 degrees) gives the best fit, but several other copulas do similarly

---

# Estimated correlation

```{r plot-copula}
cop_res <- sw |>
    mutate(tau_N = m_cop_N$tau[, 1],
        tau_F = m_cop_F$tau[, 1],
        tau_PL = m_cop_PL$tau,
        tau_G90 = m_cop_G90$tau[, 1],
        tau_G270 = m_cop_G270$tau[, 1]) |>
    pivot_longer(matches("^tau_"), names_to = "copula", names_prefix = "tau_",
        values_to = "tau")
    
cols <- palette.colors()[c(1, 2, 3, 4, 7)] |>
    setNames(unique(cop_res$copula))

cop_res |>
    ggplot(aes(x = year, y = tau, group = copula)) +
    geom_line(colour = "white", linewidth = 2) +
    geom_line(aes(colour = copula), linewidth = 1) +
    scale_colour_manual(values = cols) +
    labs(y = expression("Kendall's" ~ tau), x = NULL)
```

???

We modelled $\theta$ but is is easier to convert this to equivalent Kendall's $\tau$

Here I'm showing the results from the top few copula models, which all suggest the same thing, that the two series become gradually uncorrelated over time, which we would interpret as indications that the two major biogeochemical cycles in the lake became decoupled due to the input of nitrogen to the ctachment from atmospheric deposition

---

# Summary I

GAMs & their extensions are one way to adress many common questions asked of palaeo data

GAMs are relatively simple & familiar

Incredibly powerful models but remain interprettable

---

# Summary II

Don't be afraid to think beyond the usual quantitiative methods

Reach out to colleagues for help, collaborate, etc.

As a community we need to take statistical training more seriously for our ECRs

---

# Thank you & More ðŸ“·

ðŸ“¨ gavin@anivet.au.dk

ðŸ’» [github.com/gavinsimpson/inqua23](https://github.com/gavinsimpson/inqu23)

.center[

```{r, fig.align = "center", out.width="45%"}
knitr::include_graphics("resources/bit.ly_inqua-talk-2023.png")
```
]